{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11c00dc-5497-4987-9d3e-44055fe33947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Import de bibliotecas ---\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# --- 2. Configura√ß√£o dos caminhos ---\n",
    "# caminho da fonte\n",
    "landing_path = \"/Volumes/people_analytics/default/00_source\"\n",
    "\n",
    "# destino dos dados (tables)\n",
    "target_table_employees = \"people_analytics.default.bronze_employees\"\n",
    "target_table_attendance =\"people_analytics.default.bronze_attendance\"\n",
    "\n",
    "# caminho do checkpoint spark\n",
    "checkpoint_base = \"/Volumes/people_analytics/default/00_source/_checkpoints\"\n",
    "\n",
    "\n",
    "# --- 3. Fun√ß√£o Ingest ---\n",
    "def ingest_bronze(tabela_origem, tabela_destino, formato):\n",
    "    \n",
    "    # 1. Aponta para a PASTA (Diret√≥rio do Volume)\n",
    "    source_directory = landing_path \n",
    "\n",
    "    # 2. Define o filtro para pegar s√≥ o arquivo certo (ex: \"employees.csv\")\n",
    "    arquivo_filtro = f\"{tabela_origem}*.{formato}\"\n",
    "\n",
    "    checkpoint_path = f\"{checkpoint_base}/{tabela_origem}\"\n",
    "\n",
    "    print(f\"üöÄ Ingerindo {tabela_origem} -> {tabela_destino}...\")\n",
    "\n",
    "    # --- A. LEITURA (READSTREAM) ---\n",
    "    df = (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")                       # 1. Ativa o Autoloader\n",
    "        .option(\"cloudFiles.format\", formato)       # 2. Formato da origem (csv/json)\n",
    "        .option(\"cloudFiles.schemaLocation\", checkpoint_path)   # 3. Onde salvar o schema\n",
    "        .option(\"header\", \"true\")                   # (S√≥ afeta CSV)\n",
    "        .option(\"multiLine\", \"true\")                # (S√≥ afeta JSON)\n",
    "        .option(\"pathGlobFilter\", arquivo_filtro)   # 4. Filtra apenas o arquivo espec√≠fico dentro da pasta\n",
    "        .load(source_directory)                     # 5. Monitoramento\n",
    "        \n",
    "        # --- B. TRANSFORMA√á√ÉO (METADADOS) ---\n",
    "        .withColumn(\"data_ingestao\", current_timestamp())   # 6. Metadado de Auditoria de tempo\n",
    "        .withColumn(\"arquivo_origem\", col(\"_metadata.file_path\"))    # 7. Metadado de Rastreabilidade    \n",
    "    )\n",
    "    (\n",
    "        df.writeStream\n",
    "        .format(\"delta\")                # 8. Formato final (Delta Lake)\n",
    "        .outputMode(\"append\")           # 9. S√≥ adiciona novos dados\n",
    "        .option(\"checkpointLocation\", checkpoint_path)   # 10. Controle de estado\n",
    "        .option(\"mergeSchema\", \"true\")  # 11. Aceita colunas novas\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(tabela_destino)\n",
    "    )\n",
    "    print(f\"‚úÖ Sucesso! Tabela {tabela_destino} atualizada.\")\n",
    "\n",
    "\n",
    "# --- 3. EXECU√á√ÉO ---\n",
    "ingest_bronze(\"employees\", target_table_employees, \"csv\")\n",
    "ingest_bronze(\"attendance\", target_table_attendance, \"json\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_ingestion_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
